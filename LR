

#Linear regression assumptions: 
https://people.duke.edu/~rnau/testing.htm


#mount G Drive
from google.colab import drive
drive.mount('/content/drive')

Linear regression notes: https://kh3-ls-storage.s3.us-east-1.amazonaws.com/UPGrad/Linear%2BRegression%2BLecture%2BNotes.pdf

Nayve bayes: https://drive.google.com/file/d/1Bob1kIoSx06gE982QYOf0JxBT2b6rdCN/view

Descision tree: https://drive.google.com/drive/folders/1dtQ9jxU_xMHc9YJ3tgdEdEkWOgYFyZZY

Boosting: https://drive.google.com/file/d/1FS2g-ldURTvZRDZPwDsHm-WywjkuZyH0/view

Text: https://bayanbox.ir/view/1060725898744657072/An-Introduction-to-Statistical-Learning-with-Applications-in-Python.pdf

PI Session: XG boost, clustering, forrest descision treee.

his drive :https://drive.google.com/drive/folders/1mBnvQVmzfUx1brVp_0PB2-melE24KPwM
Amit pandey fractal: https://www.linkedin.com/in/amitpandeyprofile/

1. out of bag error (random forest)
2. https://www.aidancooper.co.uk/a-non-technical-guide-to-interpreting-shap-analyses/
3. PCA: Elbow method
4. agnomerative? debigue clusters?

SVM notes: https://kh3-ls-storage.s3.us-east-1.amazonaws.com/UPGrad/Lecture%2BNotes%2B-%2BSVM.pdf
lr dUKE uni: https://people.duke.edu/~rnau/411home.htm

Common Functions in Models: https://www.mathsisfun.com/sets/functions-common.html
